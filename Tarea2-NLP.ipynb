{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fb6e78b-3505-41d0-b87b-3dea251373ab",
   "metadata": {},
   "source": [
    "* **Jairo Nicolás Gómez**\n",
    "* **Juan Pablo Martinez**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd6391c-1116-4a7b-a3d6-41bac8f58eda",
   "metadata": {},
   "source": [
    "# **20N**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2998e77a-7d64-4144-b497-ae10c4f22b2b",
   "metadata": {},
   "source": [
    "## **1) Read files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6216f82-0768-4295-a839-6b917aa99641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "root_folder = \"./20news-18828\"\n",
    "# Read files and write them in a txt called output.txt\n",
    "def read_files_in_folder(folder_path):\n",
    "    lista = []\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='latin-1') as file:\n",
    "                        file_contents = file.read().replace('\\n', ' ').replace('\\t', ' ')\n",
    "                        lista.append(file_contents)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    diccionario = {}\n",
    "    for index,words in enumerate(lista):\n",
    "        sentences = words.split(\".\")\n",
    "        sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "        new_list = []\n",
    "        for ind in sentences:\n",
    "            if len(ind) > 10:\n",
    "                new_list.append(ind)\n",
    "                diccionario[index] = new_list\n",
    "    return diccionario\n",
    "\n",
    "dictionary_documents_20news = read_files_in_folder(root_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b55a97f4-4caf-4e34-99a8-6a38752138ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: baden@sys6626', \"ca (baden de bari) Subject: !!!] HELP!  NEED 3479P!!! [!!!                   Would anyone have a few extra 3479P's lying around that I  could buy off of them\", \"Problem is that around here I can only perchase  them in $30 quantities, and I don't need this, and can't take this  financially right now\", 'If anyone can accomodate me with this PLEASE  reply to BOTH for the following mailing addresses', 'Thanks!!!   _______________________________________________                  |                                 |    _______   |    Baden de Bari                |   /       \\\\  |     baden@sys6626', 'ca   |  (| o   o |) |     baden@inqmind', 'ca   |   |   ^   |  | >> True life can only           |   \\\\  -=-  /  | >> be experianced by            |    \\\\_____/   | >> those who do not fear death', '| -----------------------------------------------']\n"
     ]
    }
   ],
   "source": [
    "print(dictionary_documents_20news[12345])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82adeb60-32ad-4241-bfee-154ec464df7f",
   "metadata": {},
   "source": [
    "## **2)Tokenize by sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bff6ce51-9bf0-41b7-970c-2f31f5cf7519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "223d7797-8caa-4820-89a6-aaf03fab7d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "#Loop through the dictionary of files and do the preproccessing. Return list of sentences\n",
    "def preprocess(documents):\n",
    "\n",
    "    for index,keys in enumerate(documents):\n",
    "        sentences = documents[keys]\n",
    "        #Eliminate punctuation\n",
    "        tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "        word_tok_nltk = [tokenizer.tokenize(doc) for doc in sentences]\n",
    "        #Stemming\n",
    "        p_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "        nltk_stemedList = [[p_stemmer.stem(word) for word in doc] for doc in word_tok_nltk]\n",
    "        #Replace number with token NUM\n",
    "        big_list = []\n",
    "        for lists in nltk_stemedList:\n",
    "            lists.insert(0, \"<s>\")\n",
    "            lists.append(\"</s>\")\n",
    "            numbers = []\n",
    "            for strings in lists:\n",
    "                try:\n",
    "                    turn_number = int(strings)\n",
    "                    numbers.append(\"NUM\")\n",
    "                except:\n",
    "                    numbers.append(strings)\n",
    "            big_list.append(numbers)           \n",
    "        documents[keys] = big_list\n",
    "\n",
    "    all_sentences = []\n",
    "    for key in documents:\n",
    "        list_of_lists = documents[key]\n",
    "        for sentences in list_of_lists:\n",
    "            all_sentences.append(sentences)\n",
    "                             \n",
    "    flattened_list = [item for sublist in all_sentences for item in sublist] #List of words separated\n",
    "\n",
    "    counted = Counter(flattened_list)\n",
    "\n",
    "    frequency_threshold = 2\n",
    "\n",
    "    word_frequencies_dict = dict(counted) #Dictionary with the frequencies fo every word in the corpus\n",
    "\n",
    "    final_sentences_result = []\n",
    "    dynamic_list = []\n",
    "    string = \"\"\n",
    "    for i in flattened_list:\n",
    "        if word_frequencies_dict[i] >= 2:\n",
    "            if i == \"</s>\":\n",
    "                dynamic_list.append(i)\n",
    "                string = \" \".join(dynamic_list)\n",
    "                final_sentences_result.append(string)\n",
    "                dynamic_list = []\n",
    "                string = \"\"\n",
    "            \n",
    "            else:\n",
    "                dynamic_list.append(i)\n",
    "        else:\n",
    "            dynamic_list.append('<UNK>')\n",
    "            \n",
    "            \n",
    "    return final_sentences_result\n",
    "\n",
    "procecessed_sentences = preprocess(dictionary_documents_20news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a6a659-9768-4434-a207-242e29fa3257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357587\n"
     ]
    }
   ],
   "source": [
    "print(len(procecessed_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd09487c-51bd-468f-8038-5373b0630ab5",
   "metadata": {},
   "source": [
    "## **3) Training 80% and Test 20%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0922011e-f87f-4677-91b8-8dbeb88a72b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def divide_training_test(list_sentences):\n",
    "\n",
    "    training_number = int(round((len(list_sentences)*80)/100,0))\n",
    "    filename_training = \"20N_training.txt\"\n",
    "    list_training = []\n",
    "\n",
    "\n",
    "    test_number = int(round((len(list_sentences)*20/100),0))\n",
    "    filename_test = \"20N_testing.txt\"\n",
    "    list_test = []\n",
    "\n",
    "    count = len(list_sentences)-1\n",
    "    while training_number != 0:\n",
    "        index_sentence = random.randint(0,count)\n",
    "        sentence_training = list_sentences.pop(index_sentence)\n",
    "        list_training.append(sentence_training)\n",
    "        count -= 1\n",
    "        training_number -= 1\n",
    "\n",
    "    #Write in the training file\n",
    "    with open(filename_training, 'w') as file:\n",
    "        for sentence in list_training:\n",
    "            file.write(sentence + '\\n')\n",
    "        \n",
    "    while test_number != 0:\n",
    "        index_sentence = random.randint(0,count)\n",
    "        sentence_training = list_sentences.pop(index_sentence)\n",
    "        list_test.append(sentence_training)\n",
    "        count -= 1\n",
    "        test_number -= 1\n",
    "        \n",
    "    #Write in the training file\n",
    "    with open(filename_test, 'w') as file:\n",
    "        for sentence in list_test:\n",
    "            file.write(sentence + '\\n')\n",
    "    \n",
    "\n",
    "divide_training_test(procecessed_sentences)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914be17a-f474-4411-9c1b-18f963d6cf6f",
   "metadata": {},
   "source": [
    "## **N-Gram Models Laplace Smoothing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afda2df-7726-4493-946e-3e7abfe7be70",
   "metadata": {},
   "source": [
    "### **Unigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39a495fe-9541-4411-ac45-a636dbdd7160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the N-Gram model for Unigrams\n",
    "def unigram():\n",
    "    corpus = []\n",
    "    with open('20N_training.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            list = line.split()\n",
    "            corpus.append(list)\n",
    "    flattened_list = [item for sublist in corpus for item in sublist] #List of words separated\n",
    "    \n",
    "    word_counts = Counter(flattened_list) #Dictionary with the ocurrences of each word from the new corpus\n",
    "    N = len(flattened_list) #Number of words from the corpus\n",
    "    V = len(word_counts) # Vocabulary\n",
    "    smoothed_probabilities = {}\n",
    "    smoothed_prob = 0\n",
    "    # Laplace smoothing\n",
    "    for word, count in word_counts.items():\n",
    "        #if word != \"<UNK>\":\n",
    "        smoothed_prob = (count + 1) / (N + V)\n",
    "        smoothed_probabilities[word] = smoothed_prob\n",
    "\n",
    "    filename_unigrams = \"20N_unigrams.txt\"\n",
    "    with open(filename_unigrams, 'w') as file:\n",
    "        for word, prob in smoothed_probabilities.items():\n",
    "            file.write(f\"'{word}': {prob}\" + '\\n')\n",
    "\n",
    "\n",
    "unigram()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9119b0e-bcb6-419b-93de-6237826a8c14",
   "metadata": {},
   "source": [
    "### **Bigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0842db9d-6351-4bd9-8a31-2ffd40a295bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the N-Gram model for Bigrams\n",
    "def bigrams():\n",
    "    corpus = []\n",
    "    with open('20N_training.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            list = line.split()\n",
    "            corpus.append(list)\n",
    "\n",
    "    individual_words = [item for sublist in corpus for item in sublist] #List of words separated\n",
    "    unigram_counts = Counter(individual_words)\n",
    "\n",
    "    bigrams = []\n",
    "    for sentence in corpus:\n",
    "        for word in range(1,len(sentence)):\n",
    "            #if sentence[word] != \"<UNK>\" and sentence[word-1] != \"<UNK>\":\n",
    "            bigrams.append((sentence[word-1], sentence[word]))\n",
    "            \n",
    "    bigram_counts = Counter(bigrams)\n",
    "    \n",
    "    V = len(bigram_counts) # Vocabulary\n",
    "\n",
    "    smoothing_probabilities = {}\n",
    "\n",
    "    for bigram, frequency in bigram_counts.items():\n",
    "\n",
    "        firsterm, secondterm = bigram\n",
    "\n",
    "        unigram_counter = unigram_counts[firsterm]\n",
    "\n",
    "        smoothing_freq = (frequency + 1)/(unigram_counter + V)\n",
    "        smoothing_probabilities[bigram] = smoothing_freq\n",
    "\n",
    "        \n",
    "    filename_bigrams = \"20N_bigrams.txt\"\n",
    "    with open(filename_bigrams, 'w') as file:\n",
    "        for word, prob in smoothing_probabilities.items():\n",
    "            file.write(f\"'{word}': {prob}\" + '\\n')\n",
    "    \n",
    "\n",
    "bigrams()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dbc1cf-d42e-40c8-a207-dd6004f44a61",
   "metadata": {},
   "source": [
    "### **Trigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a12e93ed-fffd-4dbb-98b4-830c968598b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the N-Gram model for Trigrams\n",
    "def trigrams():\n",
    "    corpus = []\n",
    "    with open('20N_training.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            list = line.split()\n",
    "            corpus.append(list)\n",
    "            \n",
    "    bigrams = []\n",
    "    for sentence in corpus:\n",
    "        for word in range(1,len(sentence)):\n",
    "            #if sentence[word] != \"<UNK>\" and sentence[word-1] != \"<UNK>\":\n",
    "            bigrams.append((sentence[word-1], sentence[word]))\n",
    "            \n",
    "    bigram_counts = Counter(bigrams)\n",
    "\n",
    "    trigrams = []\n",
    "    for sentence in corpus:\n",
    "        for word in range(2,len(sentence)):\n",
    "            #if sentence[word] != \"<UNK>\" and sentence[word-1] != \"<UNK>\" and sentence[word-2] != \"<UNK>\":\n",
    "            trigrams.append((sentence[word-2],sentence[word-1], sentence[word]))\n",
    "\n",
    "    trigrams_count = Counter(trigrams)\n",
    "\n",
    "    V = len(trigrams_count)\n",
    "\n",
    "    trigrams_probabilities = {}\n",
    "\n",
    "    for trigrams, frequency in trigrams_count.items():\n",
    "        firstword, secondword, thirdword = trigrams\n",
    "        bigram_count = bigram_counts[(firstword,secondword)]\n",
    "        smoothing = (frequency + 1) / (bigram_count+V)\n",
    "        trigrams_probabilities[trigrams] = smoothing\n",
    "\n",
    "    filename_trigrams = \"20N_trigrams.txt\"\n",
    "    with open(filename_trigrams, 'w') as file:\n",
    "        for word, prob in trigrams_probabilities.items():\n",
    "            file.write(f\"'{word}': {prob}\" + '\\n')\n",
    "\n",
    "trigrams()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddfcc1d-7f3d-48ae-8576-11153b8b761e",
   "metadata": {},
   "source": [
    "# **Blogs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c71ce2-cd1b-42df-ba17-2285f1e7e8a4",
   "metadata": {},
   "source": [
    "### **Read files and preprocess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52ca8f56-70d6-42c0-8e9e-814487c628a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.9.3)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62cf15f3-e1e0-4bf2-8145-8a4bfdba6e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from os import scandir\n",
    "from re import sub\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5091cd5-0fc8-4fe2-a144-e58d6ddeefef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una carpeta llamada \"blogs\" que incluya todos los archivos .xml\n",
    "directory = 'blogs'\n",
    "\n",
    "with open('BAC_final_file.txt', 'w', errors='ignore') as bf:\n",
    "    for entry in scandir(directory):\n",
    "        if entry.is_file():\n",
    "            with open(entry.path, 'r', encoding='unicode_escape', errors='ignore') as f:\n",
    "                doc_content = f.read()\n",
    "                parser = etree.XMLParser(recover=True)\n",
    "                root = etree.fromstring(doc_content, parser=parser)\n",
    "                post_elements = root.findall(\".//post\")\n",
    "                for post_element in post_elements:\n",
    "                    post_content = post_element.text\n",
    "                    lower_string = post_content.lower() # Colocar todo el texto en minusculas\n",
    "                    no_number_string = sub(r'\\d+','NUM',lower_string) # Quitar numeros\n",
    "                    no_punc_string = sub(r'[^\\w\\s]','', no_number_string) # Quitar signos de puntuacion\n",
    "                    no_wspace_string = no_punc_string.strip() # Quitar espacios al principio y final\n",
    "                    final_string = no_wspace_string.replace('\\n','')\n",
    "                    bf.write(f'<s> {final_string} </s>\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a082f486-5b3f-440b-aa75-9f8006b82ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo consolidado para dividir los subconjuntos de entrenamiento y testing\n",
    "file_content = []\n",
    "with open('BAC_final_file.txt', 'r') as tf:\n",
    "    file_content = tf.readlines()\n",
    "\n",
    "# Cacular cuanto es el 80 % de los datos para generar el archivo de entrenamiento\n",
    "sentence_num = len(file_content)\n",
    "p80 = math.floor(sentence_num * 0.8)\n",
    "\n",
    "# Generar el archivo de training\n",
    "with open('BAC_training.txt', 'w') as btr:\n",
    "    for i in range(p80):\n",
    "        btr.write(file_content[i])\n",
    "\n",
    "# Generar el archivo de testing\n",
    "with open('BAC_testing.txt', 'w') as bts:\n",
    "    for i in range(p80+1, sentence_num):\n",
    "        bts.write(file_content[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac7afa8-fda1-4142-9c58-b5948ed8f99d",
   "metadata": {},
   "source": [
    "### **Unigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e60a152e-1802-43d6-a912-d49bb65800fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the N-Gram model for Unigrams\n",
    "def unigram_bac():\n",
    "    corpus = []\n",
    "    with open('BAC_training.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            list = line.split()\n",
    "            corpus.append(list)\n",
    "    flattened_list = [item for sublist in corpus for item in sublist] #List of words separated\n",
    "    \n",
    "    word_counts = Counter(flattened_list) #Dictionary with the ocurrences of each word from the new corpus\n",
    "    N = len(flattened_list) #Number of words from the corpus\n",
    "    V = len(word_counts) # Vocabulary\n",
    "    smoothed_probabilities = {}\n",
    "    smoothed_prob = 0\n",
    "    # Laplace smoothing\n",
    "    for word, count in word_counts.items():\n",
    "        #if word != \"<UNK>\":\n",
    "        smoothed_prob = (count + 1) / (N + V)\n",
    "        smoothed_probabilities[word] = smoothed_prob\n",
    "\n",
    "    filename_unigrams = \"BAC_unigrams.txt\"\n",
    "    with open(filename_unigrams, 'w') as file:\n",
    "        for word, prob in smoothed_probabilities.items():\n",
    "            file.write(f\"'{word}': {prob}\" + '\\n')\n",
    "\n",
    "\n",
    "unigram_bac()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a384084a-a062-4caf-8af8-2b2e4b76a436",
   "metadata": {},
   "source": [
    "### **Bigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a4f15f-a7c5-4bb2-9fc0-cb0d3bd5ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the N-Gram model for Bigrams\n",
    "def bigrams_bac():\n",
    "    corpus = []\n",
    "    with open('BAC_training.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            list = line.split()\n",
    "            corpus.append(list)\n",
    "\n",
    "    individual_words = [item for sublist in corpus for item in sublist] #List of words separated\n",
    "    unigram_counts = Counter(individual_words)\n",
    "\n",
    "    bigrams = []\n",
    "    for sentence in corpus:\n",
    "        for word in range(1,len(sentence)):\n",
    "            #if sentence[word] != \"<UNK>\" and sentence[word-1] != \"<UNK>\":\n",
    "            bigrams.append((sentence[word-1], sentence[word]))\n",
    "            \n",
    "    bigram_counts = Counter(bigrams)\n",
    "    \n",
    "    V = len(bigram_counts) # Vocabulary\n",
    "\n",
    "    smoothing_probabilities = {}\n",
    "\n",
    "    for bigram, frequency in bigram_counts.items():\n",
    "\n",
    "        firsterm, secondterm = bigram\n",
    "\n",
    "        unigram_counter = unigram_counts[firsterm]\n",
    "\n",
    "        smoothing_freq = (frequency + 1)/(unigram_counter + V)\n",
    "        smoothing_probabilities[bigram] = smoothing_freq\n",
    "\n",
    "        \n",
    "    filename_bigrams = \"BAC_bigrams.txt\"\n",
    "    with open(filename_bigrams, 'w') as file:\n",
    "        for word, prob in smoothing_probabilities.items():\n",
    "            file.write(f\"'{word}': {prob}\" + '\\n')\n",
    "    \n",
    "\n",
    "bigrams_bac()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f76531d-cdb9-4e47-96e9-ddebabacc6cc",
   "metadata": {},
   "source": [
    "### **Trigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7e6ad7-c1c5-445d-bb22-07fd533be225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the N-Gram model for Trigrams\n",
    "def trigrams_bac():\n",
    "    corpus = []\n",
    "    with open('BAC_training.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            list = line.split()\n",
    "            corpus.append(list)\n",
    "            \n",
    "    bigrams = []\n",
    "    for sentence in corpus:\n",
    "        for word in range(1,len(sentence)):\n",
    "            #if sentence[word] != \"<UNK>\" and sentence[word-1] != \"<UNK>\":\n",
    "            bigrams.append((sentence[word-1], sentence[word]))\n",
    "            \n",
    "    bigram_counts = Counter(bigrams)\n",
    "\n",
    "    trigrams = []\n",
    "    for sentence in corpus:\n",
    "        for word in range(2,len(sentence)):\n",
    "            #if sentence[word] != \"<UNK>\" and sentence[word-1] != \"<UNK>\" and sentence[word-2] != \"<UNK>\":\n",
    "            trigrams.append((sentence[word-2],sentence[word-1], sentence[word]))\n",
    "\n",
    "    trigrams_count = Counter(trigrams)\n",
    "\n",
    "    V = len(trigrams_count)\n",
    "\n",
    "    trigrams_probabilities = {}\n",
    "\n",
    "    for trigrams, frequency in trigrams_count.items():\n",
    "        firstword, secondword, thirdword = trigrams\n",
    "        bigram_count = bigram_counts[(firstword,secondword)]\n",
    "        smoothing = (frequency + 1) / (bigram_count+V)\n",
    "        trigrams_probabilities[trigrams] = smoothing\n",
    "\n",
    "    filename_trigrams = \"BAC_trigrams.txt\"\n",
    "    with open(filename_trigrams, 'w') as file:\n",
    "        for word, prob in trigrams_probabilities.items():\n",
    "            file.write(f\"'{word}': {prob}\" + '\\n')\n",
    "\n",
    "trigrams_bac()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
